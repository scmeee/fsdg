{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.0","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Kaggle-llama.cpp.ipynb)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n%cd Sakura-13B-Galgame\n\n!pip install \"diskcache>=5.6.1\"\n!pip install llama-cpp-python -i https://sakurallm.github.io/llama-cpp-python/whl/cu121\n!pip install -q -r requirements.llamacpp.txt\n!pip install -q pyngrok\n\n# install localtunnel\n!npm install -g localtunnel","metadata":{"execution":{"iopub.execute_input":"2023-12-27T12:39:24.065177Z","iopub.status.busy":"2023-12-27T12:39:24.064891Z","iopub.status.idle":"2023-12-27T12:44:04.909267Z","shell.execute_reply":"2023-12-27T12:44:04.907937Z","shell.execute_reply.started":"2023-12-27T12:39:24.065151Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ngrokToken留空则使用localtunnel进行内网穿透\nngrokToken = \"\"\nuse_pinggy = True\nMODEL = \"sakura-14b-qwen2beta-v0.9.1-q6k\"\n\n\nfrom huggingface_hub import hf_hub_download\nfrom pathlib import Path\n\nif ngrokToken:\n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(5000)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\nelse:\n    import subprocess\n    import threading\n    def start_localtunnel(port):\n        p = subprocess.Popen([\"lt\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n\nMODEL_PATH = f\"./models/{MODEL}.gguf\"\nif not Path(MODEL_PATH).exists():\n    hf_hub_download(repo_id=\"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.1-GGUF\", filename=f\"{MODEL}.gguf\", local_dir=\"models/\")\n\n!python server.py \\\n    --model_name_or_path $MODEL_PATH \\\n    --llama_cpp \\\n    --use_gpu \\\n    --model_version 0.9 \\\n    --trust_remote_code \\\n    --no-auth","metadata":{"execution":{"iopub.execute_input":"2023-12-27T12:44:04.911814Z","iopub.status.busy":"2023-12-27T12:44:04.911507Z"},"trusted":true},"execution_count":null,"outputs":[]}]}